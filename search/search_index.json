{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>MSBA 6331 Big Data Analytics (Spring 2025)</p> <p>This site hosts the FAQs for MSBA 6331 - Big Data Analytics at the Carlson School of Management, University of Minnesota, taught by Professor De Liu. </p>"},{"location":"lab_appendix/","title":"Dualcore Dataset Reference","text":""},{"location":"lab_appendix/#tables-imported-from-mysql","title":"Tables Imported from MySQL","text":"<p>The following depicts the structure of the MySQL tables imported into HDFS using Sqoop. The primary key column from the database, if any, is denoted by bold text:</p>"},{"location":"lab_appendix/#customers","title":"customers","text":"<p>201,375 records (imported to <code>/dualcore/customers</code>) </p> Index Field Description Example 0 cust_id Customer ID <code>1846532</code> 1 fname First name <code>Sam</code> 2 lname Last name <code>Jones</code> 3 address Address of residence <code>456 Clue Road</code> 4 city City Silicon <code>Sands</code> 5 state State <code>CA</code> 6 zipcode Postal code <code>94306</code>"},{"location":"lab_appendix/#employees","title":"employees","text":"<p>61,712  records (imported to <code>/dualcore/employees</code>and later used as an external table in Hive) </p> Index Field Description Example 0 emp_id Employee ID <code>BR</code> 1 fname First name <code>Betty</code> 2 lname Last name <code>Richardson</code> 3 address Address of residence <code>123 Shady Lane</code> 4 city City <code>Anytown</code> 5 state State <code>CA</code> 6 zipcode Postal Code <code>90210</code> 7 job_title Employee\u2019s job title <code>Vice President</code> 8 email eEmail address <code>br5331404@example.com</code> 9 active Is actively employed? <code>Y</code> 10 salary Annual pay (in dollars) <code>136900</code>"},{"location":"lab_appendix/#orders","title":"orders","text":"<p>1,662,951  records (imported to <code>/dualcore/orders</code>) </p> Index Field Description Example 0 order_id Order ID <code>3213254</code> 1 cust_id Customer ID <code>1846532</code> 2 order_date Date/time of order 2013-05-31 16:59:<code>34</code>"},{"location":"lab_appendix/#order_details","title":"order_details","text":"<p>3,333,244 records (imported to <code>/dualcore/order_details</code>) </p> Index Field Description Example 0 order_id Order ID <code>3213254</code> 1 prod_id Product ID <code>1754836</code>"},{"location":"lab_appendix/#products","title":"products","text":"<p>1,114 records (imported to <code>/dualcore/products</code>) </p> Index Field Description Example 0 prod_id Product ID <code>1273641</code> 1 brand Brand name <code>Foocorp</code> 2 name Name of product 4 - port <code>USB Hub</code> 3 price Retail sales price, in cents <code>1999</code> 4 cost Wholesale cost, in cents <code>1463</code> 5 shipping_wt Shipping weight (in pounds) <code>1</code>"},{"location":"lab_appendix/#suppliers","title":"suppliers","text":"<p>66  records (imported to <code>/dualcore/suppliers</code>) </p> Index Field Description Example 0 supp_id Supplier ID <code>1000</code> 1 fname First name <code>ACME Inc.</code> 2 lname Last name <code>Sally Jones</code> 3 address Address of office <code>123 Oak Street</code> 4 city City <code>New Athens</code> 5 state State <code>IL</code> 6 zipcode Postal code <code>62264</code> 7 phone Office phone number <code>(618)555-5914</code>"},{"location":"lab_appendix/#hive-tables","title":"Hive Tables","text":"<p>The following is a record count for Hive tables that are created or queried during the hands-on exercises. Use the <code>DESCRIBE tablename</code>  command in Hive to see the table structure. </p> Table Name Record  Count cart_items <code>33,812</code> cart_orders <code>12,955</code> cart_shipping <code>12,955</code> cart_zipcodes <code>12,955</code> checkout_sessions <code>12,955</code> customers <code>201,375</code> employees <code>61,712</code> order_details <code>3,333,244</code> orders <code>1,662,951</code> products <code>1,114</code> ratings <code>21,997</code> web_logs <code>412,860</code>"},{"location":"lab_appendix/#other-data-added-to-hdfs","title":"Other Data Added to HDFS","text":"<p>The following describes the structure of other important data sets added to HDFS. </p>"},{"location":"lab_appendix/#combined-ad-campaign-data","title":"Combined Ad Campaign Data","text":"<p>(788,952 records total)</p> <p>stored in two directories: </p> <ul> <li><code>/dualcore/ad_data1</code>( 438,389  records)  </li> <li><code>/dualcore/ad_data2</code>( 350,563  records). </li> </ul> Index Field Description Example 0 campaign_id Uniquely identifies our ad <code>A3</code> 1 date Date of ad display <code>05/23/2013</code> 2 time Time of ad display <code>15:39:26</code> 3 keyword Keyword that triggered ad <code>tablet</code> 4 display_site Domain where ad shown <code>news.example.com</code> 5 placement Location of ad on Web page <code>INLINE</code> 6 was_clicked Whether ad was clicked <code>1</code> 7 cpc Cost  per click, in cents <code>106</code>"},{"location":"lab_appendix/#accesslog","title":"access.log","text":"<p>412,860  records (uploaded to <code>/dualcore/access.log</code>) </p> <p>This file is used to populate the <code>web_logs</code>table in Hive. Note that the RFC 931 and Username fields are seldom populated in log files for modern public Web sites and are ignored in our RegexSerDe. </p> Index Field Description Example 0 IP address <code>192.168.1.15</code> 1 RFC 931 (Ident) <code>-</code> 2 Username <code>-</code> 3 Date/Time <code>[22/May/2013:15:01:46 -0800]</code> 4 Request <code>\"GET /foo?bar=1 HTTP/1.1\"</code> 5 Status code <code>200</code> 6 Bytes transferred <code>762</code> 7 Referer <code>\"http://dualcore.com/\"</code> 8 User agent (browser) <code>\"Mozilla/4.0 [en] (WinNT; I)\"</code> 9 Cookie (session ID) <code>\"SESSION=8763723145\"</code>"},{"location":"prework/","title":"Prework","text":""},{"location":"prework/#sign-up-for-databricks-community-edition","title":"Sign up for Databricks Community Edition","text":"<p>Databricks is the company behind Apache Spark. The community edition offers you a cost-free environment (15GB RAM and 2 Cores) with the latest Apache Spark &amp; much more. </p> <ul> <li>To sign up for Databricks Community Edition, follow the instructions here: https://docs.databricks.com/en/getting-started/community-edition.html<ul> <li>We recommend you use UMN email for the sign up. </li> <li>On the page, \"How do you plan on using Databricks?\", choose the Community Edition.</li> <li>At the final page, be sure to choose the \"Get started with Community Edition\" (the grey button instead of the red button) -- see a picture below. </li> <li>As suggested, after signing in, you may try the Get started: Query and visualize data from a notebook to familiarize yourself with Databricks.</li> </ul> </li> </ul> <p></p> <ul> <li>After that, you can bookmark this page community.cloud.databricks.com for future log on.</li> </ul>"},{"location":"prework/#linux-shell-commands","title":"Linux Shell Commands","text":"<p>We will spend some of our course time on Linux shell commands, which are often used in big data, cloud computing, and MLOps. It would be helpful for you to warm up to the concepts and commands with the following video and short web course. </p> <p>Here are some materials that warm you up to the concepts:</p> <ul> <li>Learning the Shell (web tutorial): just focus on <code>Learning the Shell</code> chapter</li> <li>Introduction to Linux Operating System (A gentle overview): only need to watch the segment of 18:05-52:03 (or the following topics)<ul> <li>18:05 Introduction to Linux operating system and comparison with windows</li> <li>24:32 Terminal vs. File Manager</li> <li>27:20 Command Line Interfaces on Ubuntu Operating system</li> <li>49:19 Brief of Linux commands</li> </ul> </li> </ul>"},{"location":"prework/#some-useful-concepts-in-computing","title":"Some Useful Concepts in Computing","text":"<p>Big data is about completing tasks in parallel, using multiple computers. Naturally, it needs some background on how computing is done on a single computer.  These are a series of short videos explaining various concepts and aspects of computing.</p> <ul> <li>https://www.youtube.com/watch?v=p3q5zWCw8J4 (how do computer memory work)</li> <li>https://www.youtube.com/watch?v=H_M--weEzpA (Memory versus Storage)</li> <li>https://www.youtube.com/watch?v=IIvbEn54ZAY (CPUs and Cores)</li> <li>https://www.youtube.com/watch?v=H4l42nbYmrU (video explaining ASCII files)</li> <li>https://www.youtube.com/watch?v=v7IpCq5YL68 (comparing plain text and binary files)</li> <li>https://www.youtube.com/watch?v=BKgRaHMUul0 (explaining binary files and demonstrating difference in file types)</li> </ul>"},{"location":"FAQs/addimage/","title":"Embed Images to Jupyter Notebook","text":""},{"location":"FAQs/addimage/#simplest-workflow","title":"Simplest workflow","text":"<ol> <li>upload the image (most commonly jpg or png) to your directory</li> <li> <p>In a markdown cell, add <code>![alternative text](path-to-image-file)</code> to display the image.</p> </li> <li> <p>Export the Jupyter Notebook to PDF and submit the PDF</p> </li> </ol>"},{"location":"FAQs/addimage/#in-the-event-that-pdf-converter-encounters-an-error-you-may-either","title":"In the event that PDF converter encounters an error, you may either","text":""},{"location":"FAQs/addimage/#embed-image-in-html","title":"Embed image in html","text":"<ol> <li> <p>convert your image using base64 encoder using online services https://www.base64-image.de/ </p> </li> <li> <p>In a markdown cell, enter an image tag <code>&lt;img src=\"data:image/png;base64\"/&gt;</code>, and replace the <code>data:image/...</code> portion with your encoded image from your encoder. The image should display in the rendered markdown cell. </p> </li> <li> <p>Export the HTML and submit it.</p> </li> </ol> <p>The reason that we have to embed the image is because HTML files link to images rather than embed them by default. Even though you have uploaded your image to your workspace, I cannot access this file in your space. So when you convert it to HTML and view it in a place other than your server, the image cannot be displayed. Embedding forces the HTML to store the image data within the HTML file.</p>"},{"location":"FAQs/addimage/#submit-html-and-images-separately","title":"Submit HTML and image(s) separately.","text":"<ol> <li>Alternatively, you can submit your HTML as well as your images to Canvas.</li> </ol>"},{"location":"FAQs/cheatsheet_git/","title":"Git Cheatsheet","text":"<p>Note: you may need to prefix git commands with <code>sudo</code> when you use git in the VM environment. <code>sudo</code> give you the necessary privilege for certain git operations.</p> <p>clone the repository</p> <pre><code># accept default folder name [vagrant]\ngit clone https://github.umn.edu/deliu/vagrant\n\n# use your own folder name\ngit clone https://github.umn.edu/deliu/vagrant myfoldername\n\n# On VM, you need to provide your userid and add sudo, you'll be asked your U of M password\nsudo git clone https://myuserid@github.umn.edu/deliu/vagrant\n</code></pre> <p>Add files to the repository</p> <pre><code>git add file1.txt\ngit add file2.gif\ngit add *.py\n</code></pre> <p>Add all (new and changed) files to the repository</p> <pre><code>git add -A\n</code></pre> <p>Review your repository status</p> <pre><code>git status\n</code></pre> <p>Commit the changes (the description is required)</p> <pre><code>git commit -m \"Description of my change\"\n</code></pre> <p>Push them back to the remote repository</p> <pre><code>git push\n</code></pre> <p>Get latest version - you should do this before you make your changes</p> <pre><code>git pull\n</code></pre> <p>To undo changes you have done to the files (that you have not committed). </p> <pre><code>git checkout\n</code></pre>"},{"location":"FAQs/configGit/","title":"Config Git","text":"<p>After you install Git, it takes a few simple steps to begin using it:</p> <p>At the command line:</p> <pre><code># configure your name\ngit config --global user.name \"myname\"\n# email\ngit config --global user.email \"myemail@umn.edu\"\n# when you type your password, it is cache for long time\ngit config --global credential.helper \"cache --timeout=99999\"\n</code></pre>"},{"location":"FAQs/debug_hadoop/","title":"Debug MapReduce Jobs","text":"<p>When you work with Hadoop MapReduce jobs for this course, here are some of the tips for debugging your Hadoop MapReduce issues:</p> <ol> <li>Do your input files exist, and in the format expected?<ul> <li>You can use <code>hadoop fs -cat file | head</code> command to find this out.</li> </ul> </li> <li>Does your output folder already exist?<ul> <li>Hadoop won't run if  the output folder already exists.</li> <li>Use <code>hadoop fs -rm -r -f output_dir</code> to remove the output folder.</li> </ul> </li> <li>Does your python mapper/reducer have syntax or run-time errors?<ul> <li>You can debug your mapper and reducer programs using Linux pipes. </li> </ul> </li> <li>If you pass step 3, does your mapper/reducer file has wrong line endings?<ul> <li>Follow tips here to view and convert line endings.</li> </ul> </li> </ol>"},{"location":"FAQs/debug_hadoop_streaming/","title":"Debug Hadoop Streaming Jobs","text":"<p>When you work with Hadoop MapReduce streaming jobs, it may be a good idea to debug your mapper and reducer codes using Linux pipes such as below. </p> <pre><code>hadoop fs -cat input_data | head -n 100 | python mapper.py | sort | python reducer.py\n</code></pre> <p>This is how Hadoop Streaming will use these programs. This will let you quickly see bugs in your python program without the overhead of invoking MapReduce. </p> <p>Hope this helps!</p>"},{"location":"FAQs/edit_linux_file/","title":"Edit files in Linux","text":"<p>This tutorial explains how you can edit files in Linux. </p>"},{"location":"FAQs/edit_linux_file/#approach-1-using-linux-command-line-tools","title":"Approach 1. Using Linux command line tools.","text":"<p>In our environment, you can use command-line based text editors including <code>nano</code> and <code>vi</code> to edit something small. <code>vi</code> is briefly explained. <code>nano</code> is relative intuitive to learn and use.</p> <p>You can type:</p> <p><code>nano file.txt</code></p> <p>to open or create a file. Edit it, and then follow the shortcut at the bottom of your screen for file operations, including</p> <ul> <li>Ctrl+O: write out (i.e. Save)</li> <li>Ctrl+X: exit Nano </li> </ul> <p>If you are using Gitbash for this, you can use your mouse to select a block of text (which is automatically copied), and right click to paste.</p>"},{"location":"FAQs/edit_linux_file/#approach-2-using-linuxs-gedit-tool","title":"Approach 2. Using Linux's Gedit tool","text":"<p>Open Virtualbox, click play to open a GUI on your running VM.</p> <p>From the menu, go to Applications &gt; Accessories &gt; Gedit text editor</p> <p>Then you can edit and save. Keep in mind that your user name and home directory is \"cloudera\". </p> <p>Gedit is versatile; it can be used edit txt files and scripts (such as python)</p>"},{"location":"FAQs/edit_linux_file/#approach-3-using-windows-tools","title":"Approach 3. Using Windows tools","text":"<p>You can take advantage of the fact that we have a shared folder between window host and the VM. This shared folder is at <code>c:/vagrant</code> on the windows side, and <code>/vagrant</code> on the Linux side. the files you put in this folder can be seen from both sides.</p> <p>Use notepad++ to edit a text file or script file and save to <code>c:/vagrant</code>.</p> <p>To ensure compatibility, it is best for you to save the text file with Linux line endings (by default it is windows line endings). To do that, go to Edit &gt; EOL conversion &gt; Unix (LF) to set it to use Linux line endings. See more about line ending conversion here.</p> <p>Once you have the file in the vagrant folder, you can easily copy or move it to a new location in Linux. e.g.</p> <p><code>cp myscript.py ~/training_materials/analyst/myscript.py</code></p> <p>The same approach can also be used for moving data files between the two systems.</p>"},{"location":"FAQs/hive_debug/","title":"Fix Hive metastore db error","text":"<p>If you encounter this error:</p> <p><code>Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient</code></p> <p>It could be either:</p> <ul> <li>You have other Hive instances (e.g. a Hive shell) running that locks the access to Hive metastore.</li> <li>Hive metastore is corrupt or not initialized properly.</li> </ul> <p>Please note that because Hive's metastore database is powered by a single-user embedded database called Derby, you cannot run two Hive instances at once. Be sure to close other instances of Hive (or Hive Shell) before you run Hive.</p> <p>Please take the follow steps:</p> <ol> <li>Close all other Hive instances (hive shells) and try again. This includes closing all notebooks and terminals but the current one. The second icon (a circle with a square hole) on the left panel will show all the running notebooks and terminals with a \"Shut down\" button.</li> <li>If the error persists, you can try to reinitialize Hive by running this hive setup script <code>~/MSBA6330/uploads/setup_hive.sh</code>. Note that, after this, you also need to reinstall the databases that you need (see relevant lab instructions)</li> <li>You may restart the course server (by going to menu: file &gt; hub control panel &gt; stop server )</li> </ol>"},{"location":"FAQs/homework-guidelines/","title":"Homework Submission Guidelines","text":""},{"location":"FAQs/homework-guidelines/#1-credit-sources-to-avoid-plagiarism","title":"1. Credit sources to avoid plagiarism","text":"<p>For short answers, if you use others' work as part of your answer, please properly cite your source. While it is fine to use web or other sources as a reference, you are prohibited from to lift whole paragraphs from the Internet. This is considered plagiarism, especially when you do not indicate the boundary of the copied content. When you quote a whole sentence or more, please add quotation marks around the copied content and indicate sources, so that we know you did not write that. </p> <p>Please refer to the following example for inline citation and bibliography styles. Please use the author-year format for inline citations. </p> <p>This phenomenon has been mentioned in several sources include a web page <sup>1</sup> and a journal paper (Yeh 1996). <sup>2</sup></p>"},{"location":"FAQs/homework-guidelines/#2-generative-ai-policy","title":"2. Generative AI Policy","text":"<p>Generative AI (Gen AI) tools, such as ChatGPT, Copilot, DALL-E, Bard, and others, can be valuable resources for enhancing your learning experience. In this course, we also provide a SmartPal chatbot for course-specific Gen AI support. However, their use must align with the following guidelines to maintain academic integrity and ensure meaningful learning:</p> <p>You are encouraged to use Gen AI for non-exam assignments, including homework. You may use it to explore ideas, brainstorm solutions, or enhance your understanding.</p> <p>That said, you should use Gen AI responsibly:</p> <ul> <li>Accuracy and Relevance: AI outputs can be inaccurate or irrelevant. Always verify and refine the generated content to ensure it aligns with the course material and the specific question or context.</li> <li>Understanding: You must fully understand and be able to explain all submitted work. If asked during exams or discussions, you should demonstrate knowledge of the content without AI assistance.</li> <li>Original Perspectives: Incorporate your own ideas and analysis. Gen AI should support, not replace, your critical thinking and individual contribution.</li> </ul>"},{"location":"FAQs/homework-guidelines/#3-submission-format","title":"3. Submission format","text":"<p>Submit all your answers in acceptable file formats (html, pdf, docx), preferably in one file. We do not accept ipynb because Canvas does not know how to render it.</p> <p>On Databricks, you can export your notebook as HTML and submit the HTML file.</p> <p>If your submission includes multiple files, attach them individually (do not zip it). </p>"},{"location":"FAQs/homework-guidelines/#4-how-include-an-image-in-your-submission","title":"4. How include an image in your submission","text":"<p>If you need to upload a picture, we suggest you to upload the image to a free image hosting site https://freeimage.host/, and then copy the Markdown Full linked code into a markdown cell (a cell started with <code>%md</code>, as shown below, where the second line is coped from freeimage.host).</p> <p></p>"},{"location":"FAQs/homework-guidelines/#5-how-to-take-a-screenshot","title":"5. How to take a screenshot","text":"<p>Occasionally you may need to submit a screenshot. Here are some tips</p> <p>Windows : In windows 7 or above, you may use the built-in Snipping tool , which allows you to select an area on screen and capture it. In Windows 10, you may use shortcut \"Win+Shift+S\" to select a screen area to save.</p> <p>Mac : On Mac OS, Press Command-Shift-4, and then drag the cross-hair pointer to select the area. </p> <ol> <li> <p>\"Zen and the Art of the Internet.\" http://freenet.buffalo.edu/~popmusic/zen10.txt\u00a0\u21a9</p> </li> <li> <p>Yeh, Michelle. \"The 'Cult of Poetry' in Contemporary China.\" Journal  of Asian Studies  55 (1996): 51-80.\u00a0\u21a9</p> </li> </ol>"},{"location":"FAQs/installLocalSpark/","title":"Install Spark Locally on your Own Computer","text":"<p>We mainly used a virtual machine for Machine Learning/Data Science tasks on Github to setup a local Spark environment.</p> <p>Below are the steps</p> <ol> <li>First visit the git repository for the Spark Virtual Machine (VM) and take a look what this virtual machine offers.</li> <li>Set up the Spark directory on your computer by either using git clone (if you have git installed on your machine), or by downloading a copy of the above git repository. To avoid trouble, we advise you use a directory name without spaces. I recommend use <code>c:/spark</code> for windows machines.</li> <li>Install the latest version of vagrant at  https://www.vagrantup.com/downloads.</li> <li>Install the VirtualBox 6.0 version at https://www.virtualbox.org/wiki/Download_Old_Builds_6_0. Please note that the newest version of VirtualBox may not be supported by vagrant yet. </li> <li>Provision the virtual machine from a command window by CD the directory you have the Spark vm files first (e.g. <code>c:/spark</code>), then execute the following command: <pre><code># brings up the VM\nvagrant up\n</code></pre></li> </ol> <p>This will initialize and start the VM, which will take several minutes (subsequent booting time will be shorter). It downloads a base VM from the Internet and then augments it with the script specified in  the <code>Vagrantfile</code> in the Spark folder.</p> <ol> <li>If the provision succeeded, you can bring up a PySpark notebook by entering localhost:8008 in a browser window. Enter \"vmuser\" for both user name and password. </li> </ol> <p>You may upload a new notebook using the upload button from the jupyter notebook. The uploaded notebook will be located at  <code>c:\\spark\\vmfiles\\IPNB\\</code> (assuming your spark folder is <code>c:/spark</code>) on your computer. </p> <p>Please note that when you open the notebook, you will most likely need to manually specify the kernel to be \"PySpark (py3)\" to use Spark functionalities. </p> <p></p>"},{"location":"FAQs/installLocalSpark/#faqs","title":"FAQs","text":""},{"location":"FAQs/installLocalSpark/#1-virtualization-vt-x-is-disabled-on-your-host-machine","title":"1. Virtualization (VT-x) is disabled on your host machine","text":"<p>During the VM installation, you may fail to bring up the VM with an error message like one of the following:</p> <p>VT-x is disabled in the BIOS for all CPU modes. Binary translation is incompatible with long mode on this platform. Long mode will be disabled in this virtual environment. This virtual machine is configured for 64-bit guest operating systems. However, 64-bit operation is not possible.</p> <p>The likely cause is that Virtualization (VT-x) is not enable on your machine. VirtualBox will not play the VM if the virtulization (<code>VT-x</code>) is disabled on your host machine. </p> <p>To enable virtualization, you need to access the BIOS of your machine. Depending on the machine model, it may requires you to long press a function key such as F8, F12, and F2 during boot up. You should search for \"how to access BIOS [YOUR COMPUTER MODEL]\" for instructions specific to your computer model. After getting into to BIOS setup, you should look for an configuration that mentions <code>Intel Virtual Technology</code>, <code>VT-x</code>, <code>Virtualization</code> from the menu, then enable it. </p>"},{"location":"FAQs/line_endings/","title":"Issues with Line Endings in Text Files","text":"<p>Windows text files by default have line endings of <code>\\r\\n</code>, but linux/unix text files have line endings of <code>\\n</code>. Often times, files originated from windows may not work for linux because of this (e.g. for python-based mappers and reducers). </p>"},{"location":"FAQs/line_endings/#how-would-i-know-the-line-endings-were-rn-instead-of-n","title":"How would I know the line endings were <code>\\r\\n</code> instead of <code>\\n</code>?","text":"<p>A quick way to see if the file has windows line ending is through vim. <pre><code>vi file.py \n</code></pre> type <code>:set list</code> to see line endings.  If it is windows line endings, you will see <code>^M</code> at end of each line. Type <code>:q</code> to exit the vi editor.</p>"},{"location":"FAQs/line_endings/#how-to-fix-it","title":"How to fix it?","text":"<p>To replace windows line endings with linux ones, use linux command: <pre><code>sed -i 's/\\r//g' file.py\n</code></pre></p>"},{"location":"FAQs/line_endings/#how-to-prevent-this","title":"How to prevent this?","text":"<p>If you have to edit a file in windows, please create the file in Linux or copy an existing linux-originated file (so it has the right endings), then edit it.</p> <ul> <li>Notepad++: You can view line endings using tool bar button \"\u00b6\". At the status bar, you can switch between \"Unix(LF)\" and \"Windows (CR LF)\".</li> <li>Sublime Text: This page explains how to set default line endings to linux. Here is how you may convert it.</li> </ul>"},{"location":"FAQs/markdown2pdf/","title":"Convert Markdown to PDF","text":"<p>Our assignments are distributed in markdown files. If you want to leverage the markdown file, you will need a workflow that converts markdown to pdf. Here are some of the options:</p> <ul> <li>Convert Markdown to PDF</li> <li>1. Online Markdown to PDF Converter</li> <li>2. R-Studio - Requires Pandoc</li> <li>3. Jupyter</li> <li>4. Specialized Markdown Editor</li> <li>5. Sublime Text + Markdown Preview + Chrome</li> </ul> <p></p>"},{"location":"FAQs/markdown2pdf/#1-online-markdown-to-pdf-converter","title":"1. Online Markdown to PDF Converter","text":"<p>The easiest is perhaps the cloud-based markdown to PDF converters</p> <ul> <li>https://cloudconvert.com/md-to-pdf: multipurpose converter including md to pdf.</li> <li>Dillinger.io: editing and expert to pdf, html</li> <li>Markdowntopdf.com: quick and easy</li> </ul> <p></p>"},{"location":"FAQs/markdown2pdf/#2-r-studio-requires-pandoc","title":"2. R-Studio - Requires Pandoc","text":"<p>You can open the <code>md</code> file in R-studio, and save as PDF (this is done through Pandoc).</p> <p>To install Pandoc, you can download it from Pandoc. Note that Latex software, such as MikTex is also required.</p> <p></p>"},{"location":"FAQs/markdown2pdf/#3-jupyter","title":"3. Jupyter","text":"<p>You can open the <code>md</code> file in jupyter notebook, and save as PDF (this is done through Pandoc).</p> <p>Alternatively, you may choose to print the file, then using the browser's built-in save as PDF capability. Specifically, go to File &gt; Print Preview &gt; Using the Browser's Save As PDF (e.g. Chrome) or Print + Microsoft Print to PDF (Internet Explorer). </p> <p></p>"},{"location":"FAQs/markdown2pdf/#4-specialized-markdown-editor","title":"4. Specialized Markdown Editor","text":"<p>Specialized Markdown Software such as Typora or Markdown Monster tend to have their own markdown to PDF converter.</p> <p></p>"},{"location":"FAQs/markdown2pdf/#5-sublime-text-markdown-preview-chrome","title":"5. Sublime Text + Markdown Preview + Chrome","text":"<p>If use sublime text as your text editor. There are markdown editing and preview packages you can install.</p> <ul> <li>Use sublime package control to install<ul> <li>MarkdownEditing</li> <li>Markdown Preview</li> </ul> </li> <li>Alt+M to view markdown in Chrome, Ctrl+p to print to PDF.</li> </ul>"},{"location":"FAQs/mounts3/","title":"Mount s3 folder to databricks","text":"<p>Databricks community edition has 10G limitation on how much data you can upload to databricks DBFS. If you have a bigger files you can upload to Amazon's S3 and mount it on Databricks.</p> <p>1. Create an Amazon S3 policy</p> <p>Go to AWS's IAM control panel &gt; Policy &gt; Create policy &gt; Go to JSON (tab),  &gt; paste the following policy and change <code>msbabigdata</code> (in two places) to your own bucket name. This policy gives the policy holder full access to your s3 folder.</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:ListBucket\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::msbabigdata\"\n            ]\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:PutObject\",\n                \"s3:GetObject\",\n                \"s3:DeleteObject\",\n                \"s3:PutObjectAcl\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::msbabigdata/*\"\n            ]\n        }\n    ]\n}\n</code></pre> <p>Then give the policy a name \"s3databricks\", and Create policy.</p> <p>2. Create an IAM user and attach the policy</p> <p>Go to IAM &gt; Users &gt; Add user </p> <p>Give a user name (e.g. <code>s3databricks</code>), check \"Programmatic access\", \"Next: Permissions\"</p> <p>Click \"Attach existing policies directly\", search for \"s3databricks\" that you created, check this policy, then click \"Next:tags\"</p> <p>Accept the defaults in the next few screens, until you \"Create user\". </p> <p>Download the credential \".csv\" and save it in a safe place (you can only download it once). You will need the Access key ID, and Secret later. You can show the Secret and copy it for later use. </p> <p>3. Mount the s3 bucket in Databricks</p> <p>In a Databricks notebook, paste the following and replace the <code>ACCESS_KEY</code> and <code>SECRET_KEY</code> with the Access key ID and Secret you obtained in the above. </p> <p>Also replace the <code>AWS_BUCKET_NAME</code> with your bucket name. You may choose your own mount folder name.</p> <pre><code>ACCESS_KEY = \"xxx\"\nSECRET_KEY = \"xxxxx\"\nENCODED_SECRET_KEY = SECRET_KEY.replace(\"/\", \"%2F\")\nAWS_BUCKET_NAME = \"msbabigdata\"\nMOUNT_NAME = \"msbabigdata\"\ntry:\n  dbutils.fs.unmount( \"/mnt/%s\" % MOUNT_NAME)\nexcept:\n  pass\ndbutils.fs.mount(\"s3a://%s:%s@%s\" % (ACCESS_KEY, ENCODED_SECRET_KEY, AWS_BUCKET_NAME), \"/mnt/%s\" % MOUNT_NAME)\n</code></pre> <p>Remark: in general you do not want to directly provide your secret in your code , but the safer approach uses a Databricks API that is disabled in the community edition.</p> <p>Note that we did an unmount first in case you want to remount it.</p> <p>Then you may test your mounted s3 folder using DBFS utility:</p> <pre><code>%fs ls /mnt/msbabigdata\n</code></pre> <p>Later you may reference folders/files in this folder using</p> <p>\"/mnt/msbabigdata/...\"</p> <p>e.g. </p> <p><code>myRDD = sc.textFile(\"/mnt/msbabigdata/adirdata/ratings_2013.txt\")</code></p>"},{"location":"FAQs/sparkfaq/","title":"Common Issues with PySpark","text":"<ul> <li>Common Issues with PySpark<ul> <li>What if the data file I try to upload through Jupyter Notebook exceeds 25 MB?</li> </ul> </li> <li>if you encounter an error, <code>The root scratch dir: /tmp/hive on HDFS should be writable</code></li> <li>Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient</li> </ul>"},{"location":"FAQs/sparkfaq/#what-if-the-data-file-i-try-to-upload-through-jupyter-notebook-exceeds-25-mb","title":"What if the data file I try to upload through Jupyter Notebook exceeds 25 MB?","text":"<p>Jupyter Notebook allows you to upload files no larger than 25MB. If the file exceeds the 25 MB, you can use the wget approach (if the data file has a downloadable link). </p> <p>To use <code>wget</code> from jupyter notebook, use <code>New &gt; Terminal</code>, then you have access to the bash terminal, where you can run:</p> <pre><code>wget url_to_data_file\n</code></pre> <p>If the file is already your disk (but no in cloudera VM), you can upload to cloudera VM using the shared vagrant folder (e.g. your MSBA desktop's c:/vagrant is shared with Cloudera VM's /vagrant). </p> <ol> <li>copy your file to <code>c:/vagrant</code> on your MSBA desktop</li> <li>open bash from Cloudera VM. Type <pre><code>ls /vagrant/\n</code></pre> to verify that your file exists in the vagrant folder on your VM. </li> <li>Copy or move your file to the intended directory, e.g.:  <pre><code>cp /vagrant/file  ~/file\n</code></pre> This will copy your file to your home directory. </li> </ol> <p></p>"},{"location":"FAQs/sparkfaq/#if-you-encounter-an-error-the-root-scratch-dir-tmphive-on-hdfs-should-be-writable","title":"if you encounter an error, <code>The root scratch dir: /tmp/hive on HDFS should be writable</code>","text":"<p>If you encounter an error, <code>The root scratch dir: /tmp/hive on HDFS should be writable</code>. Open a terminal and run the following comamnd: </p> <pre><code>sudo chmod 777 /tmp/hive\n</code></pre> <p></p>"},{"location":"FAQs/sparkfaq/#unable-to-instantiate-orgapachehadoophiveqlmetadatasessionhivemetastoreclient","title":"Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient","text":"<p>If at any point you see this error, <code>AnalysisException: u'java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient;</code>. </p> <p>Please - removing the *.lck file from hive <code>metastore_db</code> <pre><code># assuming you're running this from your home directory from cloudera vm\nrm  metastore_db/*.lck\n</code></pre> - Terminate all other running jupyter notebooks (from your jupyter home, go to Running tab, then terminate). </p> <p>If the above does not work still, try to restart the kernel (from your current jupyter notebook's menu, kernel &gt; restart). </p>"},{"location":"FAQs/textEditor/","title":"Recommended Text Editors for Scripting &amp; Text File Handling","text":"<p>Much of the data science is text based, including plain text files, csv, JSON files, Markdown files, even jupyter notebook files, python/R scripts, and shell scripts. Having a powerful text editor can increase your productivity and empower you.</p> <p>Here are a few text editors that I have used and recommend. All of them are powerful editors with tons of features and abilities that can edit all kinds of text files as mentioned above. They all support syntax highlighting, multi-cursor editing, useful shortcuts, powerful search/replace features, etc.  All of them are free also. </p> <ul> <li>Visual Studio Code: Visual Studio Code (VS Code) is a newer and immensely popular text editor. It is cross-platform, modern, and inherits the best features of outstanding text/script editors such as sublime, and make them even more professionally done, extensible, and powerful. I switched from Sublime to VS Code recently for VS Code's built-in support for jupyter notebook. It has nearly all of the sublime features that I regularly use and more. Love it so far. </li> </ul> <ul> <li>Notepad++: beloved text/script editor for Windows users. It had been my primary editor for a long time before I switched to Sublime Text about 3 years ago. </li> </ul> <ul> <li>Sublime Text: text/script editor for all platforms (the evaluation version does not expire but it will ask you to purchase once in a while). It is beautiful, supports powerful multi-cursor editing, select all text of the same pattern, searching in all files, tons of plugins (and easy-to-install), easy to use command pallete, go to anything, and git integration. I have happily used for a while for all of my coding, git repositories, note taking, to-do list management, and the entire course production workflow. </li> </ul>"},{"location":"FAQs/viewdelimiters/","title":"View Delimiters and Invisible Characters","text":"<p>Often time you need to know whether a while space is a tab or space, and whether an invisible character has been used as delimiters.</p>"},{"location":"FAQs/viewdelimiters/#linux","title":"Linux","text":"<p>You can use <code>cat -t</code> including (<code>cat -t | head</code>) to show invisible characters</p> <p>e.g.:</p> <p><code>cat -t sample_artist_data.txt | head</code></p> <p><pre><code>1134999^I^A06Crazy^B Life\n6821360^IPang^C Nakarin\n10113088^ITerfel, Bartoli- Mozart: Don\n10151459^IThe Flaming Sidebur\n6826647^IBodenstandig 3000\n</code></pre> where  - <code>^I</code>: tab (<code>\\t</code>) - <code>^A</code>: control-A (<code>\\001</code>) - <code>^B</code>: control-B (<code>\\002</code>) - <code>^C</code>: control-C (<code>\\003</code>)</p>"},{"location":"FAQs/viewdelimiters/#windows","title":"Windows","text":"<p>In windows, with Notepad++, turn on the \"Show All Characters\" (menu&gt; view &gt; show symbol &gt; show all characters) </p> <p></p> <p>In sublime, if you select text, it will show invisible characters including space and tab.</p> <p></p> <p>Additionally, you may use regex search (<code>\\t</code>, <code>\\n</code>, <code>\\r</code>,<code>\\001</code> etc) to highlight special characters.</p> <p></p>"},{"location":"FAQs/windows_cmd/","title":"Windows Commands 101","text":"<p>This short tutorial introduces the basics of command line operations in Windows OS. </p>"},{"location":"FAQs/windows_cmd/#1-open-a-command-window-window","title":"1. Open a command window window","text":"<p>To open a command window at a given directory, </p> <ul> <li>Windows: go to the directory in the file explorer, Shift+Right click, you\u2019ll see \"Open a command window here\". <ul> <li>If you have git installed. You may also right-click and \"open a git bash window\", which is another flavor of command window.</li> </ul> </li> <li>Mac OS: you can enable the \"open terminal here\" service and access the service using Right Click (or Control + Click), then services (more detail here http://goo.gl/cmMmz).</li> </ul>"},{"location":"FAQs/windows_cmd/#2-commonly-used-windows-commands","title":"2. Commonly used Windows commands","text":"Purpose windows Mac OS view content of current directory <code>dir</code> <code>ls</code> <code>ls -l</code> go to a subdirectory* <code>cd dir_name</code> <code>cd dir_name</code> go one level up <code>cd ..</code> <code>cd ..</code> what is the current directory <code>dir</code> <code>pwd</code> <p>Note: in many cases, you can type tab to autocomplete the dir/file name</p>"},{"location":"FAQs/windows_cmd/#3-run-python-script-from-command-window","title":"3. Run python script from command window","text":"<pre><code>python python_file_name.py\n</code></pre>"}]}