{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"MSBA 6331 Big Data Analytics (Spring 2025)","text":"<p>This repository servers as a public homepage for MSBA 6331 - Big Data Analytics at the Carlson School, University of Minnesota. It also hosts the syllabus and FAQs. </p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> <li><code>mkdocs gh-deploy</code> - Deploy the projects to github</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"debug_hadoop/","title":"How to Debug Hadoop MapReduce Jobs","text":"<p>When you work with Hadoop MapReduce jobs for this course, here are some of the tips for debugging your Hadoop MapReduce issues:</p> <ol> <li>Do your input files exist, and in the format expected?<ul> <li>You can use <code>hadoop fs -cat file | head</code> command to find this out.</li> </ul> </li> <li>Does your output folder already exist?<ul> <li>Hadoop won't run if  the output folder already exists.</li> <li>Use <code>hadoop fs -rm -r -f output_dir</code> to remove the output folder.</li> </ul> </li> <li>Does your python mapper/reducer have syntax or run-time errors?<ul> <li>You can debug your mapper and reducer programs using Linux pipes. </li> </ul> </li> <li>If you pass step 3, does your mapper/reducer file has wrong line endings?<ul> <li>Follow tips here to view and convert line endings.</li> </ul> </li> </ol>"},{"location":"edit_linux_file/","title":"Edit linux file","text":""},{"location":"edit_linux_file/#how-to-edit-files-in-linux","title":"How to edit files in Linux","text":"<p>This tutorial explains how you can edit files in Linux. </p>"},{"location":"edit_linux_file/#approach-1-using-linux-command-line-tools","title":"Approach 1. Using Linux command line tools.","text":"<p>In our environment, you can use command-line based text editors including <code>nano</code> and <code>vi</code> to edit something small. <code>vi</code> is briefly explained in lab 0. <code>nano</code> is relative intuitive to learn and use</p> <p>You can type:</p> <p><code>nano file.txt</code></p> <p>to open or create a file. Edit it, and then follow the shortcut at the bottom of your screen for file operations, including</p> <ul> <li>Ctrl+O: write out (i.e. Save)</li> <li>Ctrl+X: exit Nano </li> </ul> <p>If you are using Gitbash for this, you can use your mouse to select a block of text (which is automatically copied), and right click to paste.</p>"},{"location":"edit_linux_file/#approach-2-using-linuxs-gedit-tool","title":"Approach 2. Using Linux's Gedit tool","text":"<p>Open Virtualbox, click play to open a GUI on your running VM.</p> <p>From the menu, go to Applications &gt; Accessories &gt; Gedit text editor</p> <p>Then you can edit and save. Keep in mind that your user name and home directory is \"cloudera\". </p> <p>Gedit is versatile; it can be used edit txt files and scripts (such as python)</p>"},{"location":"edit_linux_file/#approach-3-using-windows-tools","title":"Approach 3. Using Windows tools","text":"<p>You can take advantage of the fact that we have a shared folder between window host and the VM. This shared folder is at <code>c:/vagrant</code> on the windows side, and <code>/vagrant</code> on the Linux side. the files you put in this folder can be seen from both sides.</p> <p>Use notepad++ to edit a text file or script file and save to <code>c:/vagrant</code>.</p> <p>To ensure compatibility, it is best for you to save the text file with Linux line endings (by default it is windows line endings). To do that, go to Edit &gt; EOL conversion &gt; Unix (LF) to set it to use Linux line endings. See more about line ending conversion here.</p> <p>Once you have the file in the vagrant folder, you can easily copy or move it to a new location in Linux. e.g.</p> <p><code>cp myscript.py ~/training_materials/analyst/myscript.py</code></p> <p>The same approach can also be used for moving data files between the two systems. </p>"},{"location":"hive_debug/","title":"Hive debug","text":""},{"location":"hive_debug/#how-to-fix-hive-metastore-db-error","title":"How to Fix Hive metastore db error","text":"<p>If you encounter this error:</p> <p><code>Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient</code></p> <p>It could be either:</p> <ul> <li>You have other Hive instances (e.g. a Hive shell) running that locks the access to Hive metastore.</li> <li>Hive metastore is corrupt or not initialized properly.</li> </ul> <p>Please note that because Hive's metastore database is powered by a single-user embedded database called Derby, you cannot run two Hive instances at once. Be sure to close other instances of Hive (or Hive Shell) before you run Hive.</p> <p>Please take the follow steps:</p> <ol> <li>Close all other Hive instances (hive shells) and try again. This includes closing all notebooks and terminals but the current one. The second icon (a circle with a square hole) on the left panel will show all the running notebooks and terminals with a \"Shut down\" button.</li> <li>If the error persists, you can try to reinitialize Hive by running this hive setup script <code>~/MSBA6330/uploads/setup_hive.sh</code>. Note that, after this, you also need to reinstall the databases that you need (see relevant lab instructions)</li> <li>You may restart the course server (by going to menu: file &gt; hub control panel &gt; stop server )</li> </ol>"},{"location":"mounts3/","title":"How to Mount s3 folder to databricks","text":"<p>Databricks community edition has 10G limitation on how much data you can upload to databricks DBFS. If you have a bigger files you can upload to Amazon's S3 and mount it on Databricks.</p>"},{"location":"mounts3/#1-create-an-amazon-s3-policy","title":"1. Create an Amazon S3 policy","text":"<p>Go to AWS's IAM control panel &gt; Policy &gt; Create policy &gt; Go to JSON (tab),  &gt; paste the following policy and change <code>msbabigdata</code> (in two places) to your own bucket name. This policy gives the policy holder full access to your s3 folder.</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:ListBucket\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::msbabigdata\"\n            ]\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:PutObject\",\n                \"s3:GetObject\",\n                \"s3:DeleteObject\",\n                \"s3:PutObjectAcl\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::msbabigdata/*\"\n            ]\n        }\n    ]\n}\n</code></pre> <p>Then give the policy a name \"s3databricks\", and Create policy.</p>"},{"location":"mounts3/#2-create-an-iam-user-and-attach-the-policy","title":"2. Create an IAM user and attach the policy","text":"<p>Go to IAM &gt; Users &gt; Add user </p> <p>Give a user name (e.g. <code>s3databricks</code>), check \"Programmatic access\", \"Next: Permissions\"</p> <p>Click \"Attach existing policies directly\", search for \"s3databricks\" that you created, check this policy, then click \"Next:tags\"</p> <p>Accept the defaults in the next few screens, until you \"Create user\". </p> <p>Download the credential \".csv\" and save it in a safe place (you can only download it once). You will need the Access key ID, and Secret later. You can show the Secret and copy it for later use. </p>"},{"location":"mounts3/#3-mount-the-s3-bucket-in-databricks","title":"3. Mount the s3 bucket in Databricks","text":"<p>In a Databricks notebook, paste the following and replace the <code>ACCESS_KEY</code> and <code>SECRET_KEY</code> with the Access key ID and Secret you obtained in the above. </p> <p>Also replace the <code>AWS_BUCKET_NAME</code> with your bucket name. You may choose your own mount folder name.</p> <pre><code>ACCESS_KEY = \"xxx\"\nSECRET_KEY = \"xxxxx\"\nENCODED_SECRET_KEY = SECRET_KEY.replace(\"/\", \"%2F\")\nAWS_BUCKET_NAME = \"msbabigdata\"\nMOUNT_NAME = \"msbabigdata\"\ntry:\n  dbutils.fs.unmount( \"/mnt/%s\" % MOUNT_NAME)\nexcept:\n  pass\ndbutils.fs.mount(\"s3a://%s:%s@%s\" % (ACCESS_KEY, ENCODED_SECRET_KEY, AWS_BUCKET_NAME), \"/mnt/%s\" % MOUNT_NAME)\n</code></pre> <p>Remark: in general you do not want to directly provide your scecret in your code , but the safer approach uses a Databricks API that is disabled in the community edition.</p> <p>Note that we did an unmount first in case you want to remount it.</p> <p>Then you may test your mounted s3 folder using DBFS utility:</p> <pre><code>%fs ls /mnt/msbabigdata\n</code></pre> <p>Later you may reference folders/files in this folder using</p> <p>\"/mnt/msbabigdata/...\"</p> <p>e.g. </p> <p><code>myRDD = sc.textFile(\"/mnt/msbabigdata/adirdata/ratings_2013.txt\")</code></p>"}]}